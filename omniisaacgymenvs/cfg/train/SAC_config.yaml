params:

  seed: null  # ${...seed}

  algo:
    name: sac
  
  model:
    name: soft_actor_critic
  
  network:
    name: soft_actor_critic
    separate: True
    mlp:
      units: [256, 512, 1024, 512, 256, 128, 64, 32, 16]
      activation: elu  # relu, tanh, sigmoid, elu, selu, swish, gelu, softplus, None (or 'None')
      initializer:
        name: default  # const_initializer, orthogonal_initializer/orthogonal, glorot_normal_initializer, glorot_uniform_initializer, variance_scaling_initializer, random_uniform_initializer, kaiming_normal
        # other: depending on name
      d2rl: False
      norm_only_first_layer: False  # seems to be unused
    value_activation: None  # seems to be unused
    normalization: null  # layer_norm, batch_norm (used only if d2rl=True)
    space:  # seems to be unused
      continuous:  # seems to be unused
        # space_config  # seems to be unused
      discrete:  # seems to be unused
        # space_config  # seems to be unused
    value_shape: 1  # seems to be unused
    central_value: False  # seems to be unused
    joint_obs_actions: null  # seems to be unused
    log_std_bounds: [-6.28, 6.28]  # null  # [log_std_min, log_std_max]

  config:
    multi_gpu: False

    reward_shaper:
      scale_value: 1
      shift_value: 0
      # min_val: -inf
      # max_val: inf
      is_torch: True
    
    features:
      observer: null
    
    actor_lr: 0.0005
    actor_betas: [0.9, 0.999]
    critic_lr: 0.0005
    critic_betas: [0.9, 0.999]
    alpha_lr: 0.005
    alphas_betas: [0.9, 0.999]

    max_epochs: ${resolve_default:1000000,${....max_iterations}}
    games_to_track: 100
    name: ${resolve_default:SAC_training,${....experiment}}
    # score_to_win: 25  # inf

    num_warmup_steps: 100  # it actually is a num_warmup_epochs not steps
    gamma: 0.99
    critic_tau: 0.005
    batch_size: 4096
    init_alpha: 1.0
    learnable_temperature: True
    replay_buffer_size: 4000000
    num_steps_per_episode: 48  # 1000
    normalize_input: False
    max_env_steps: 1000000000  # 1000
    target_entropy_coef: 1.0

    num_actors: ${....task.env.numEnvs}
    env_name: rlgpu
    
    # env_info:
    #   observation_space:
    #   agents: 1
    #   action_space:

    device: ${....rl_device}
    weight_decay: 0.0  # seems to be unused
    is_train: True  # seems to be unused
    save_best_after: 100
    print_stats: True
    population_based_training: False
    pbt_idx: 0
    full_experiment_name: null
    train_dir: runs

    env_config:
      seed: null  # ${..seed}
